{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-crawl-300d-2m', 'jigsaw-unintended-bias-in-toxicity-classification', 'glove840b300dtxt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing import text, sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode='run'\n",
    "valid=True\n",
    "k_fold=True\n",
    "n_fold=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_emb_path='../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec'\n",
    "glove_emb_path='../input/glove840b300dtxt/glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models=1\n",
    "lstm_units=128\n",
    "dense_hidden_units=4*lstm_units\n",
    "max_len=220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeffs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "def load_embeddings(path):\n",
    "    f=open(path)\n",
    "    return(dict(get_coeffs(*line.strip().split(' ')) for line in tqdm(f)))\n",
    "def build_matrix(word_index,path):\n",
    "    unknown_words=[]\n",
    "    embedding_index=load_embeddings(path)\n",
    "    embedding_matrix=np.zeros((len(word_index)+1,300))\n",
    "    for word,i in word_index.items():\n",
    "           try:\n",
    "                embedding_matrix[i]=embedding_index[word]\n",
    "           except KeyError:\n",
    "                unknown_words.append(word)\n",
    "    return(embedding_matrix, unknown_words)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable progress bars when submitting\n",
    "def is_interactive():\n",
    "    return 'SHLVL' not in os.environ\n",
    "\n",
    "if not is_interactive():\n",
    "    def nop(it, *a, **k):\n",
    "        return it\n",
    "\n",
    "    tqdm = nop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test=pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id            ...             toxicity_annotator_count\n",
       "0  59848            ...                                    4\n",
       "1  59849            ...                                    4\n",
       "2  59852            ...                                    4\n",
       "3  59855            ...                                    4\n",
       "4  59856            ...                                   47\n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\",\n",
    "                       \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n",
    "                       \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\",\n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\",\n",
    "                       \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "                       \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\",\n",
    "                       \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
    "                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\",\n",
    "                       \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n",
    "                       \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "                       \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                       \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n",
    "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \n",
    "                       \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                       \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                       \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \n",
    "                       \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                       \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                       \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text,mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for char in specials:\n",
    "        text=text.replace(char,\"‘\")\n",
    "    text=text.lower()\n",
    "    text=text.replace(\" i \",\" I \")\n",
    "    text=' '.join([mapping[t] if t in mapping.keys() else t for t in text.split(' ')])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          This is so cool. It's like, 'would you want yo...\n",
       "1          Thank you!! This would make my life a lot less...\n",
       "2          This is such an urgent design problem; kudos t...\n",
       "3          Is this something I'll be able to install on m...\n",
       "4                       haha you guys are a bunch of losers.\n",
       "5                                       ur a sh*tty comment.\n",
       "6                                hahahahahahahahhha suck it.\n",
       "7                                        FFFFUUUUUUUUUUUUUUU\n",
       "8          The ranchers seem motivated by mostly by greed...\n",
       "9          It was a great show. Not a combo I'd of expect...\n",
       "10                                   Wow, that sounds great.\n",
       "11         This is a great story. Man. I wonder if the pe...\n",
       "12            This seems like a step in the right direction.\n",
       "13         It's ridiculous that these guys are being call...\n",
       "14         This story gets more ridiculous by the hour! A...\n",
       "15         I agree; I don't want to grant them the legiti...\n",
       "16         Interesting. I'll be curious to see how this w...\n",
       "17                           Awesome! I love Civil Comments!\n",
       "18         I'm glad you're working on this, and I look fo...\n",
       "19         Angry trolls, misogynists and Racists\", oh my....\n",
       "20         Nice to some attempts to try to make comments ...\n",
       "21         One would hope that the purpose of introducing...\n",
       "22         Comments will be randomly chosen and be review...\n",
       "23         She would be a major improvement for city coun...\n",
       "24         I agree! Comments have so much potential to be...\n",
       "25         Great question! It's one we're asked a lot. We...\n",
       "26         Thanks, Christa!  Will you be adding any featu...\n",
       "27         Our aim is actually the opposite: we want spir...\n",
       "28         Thanks! We're really going to try — not only t...\n",
       "29         I applaud Civil's efforts to create some new t...\n",
       "                                 ...                        \n",
       "1804844    One of the first things that they teach in law...\n",
       "1804845    I would ask Ms. Renzetti to be honest with her...\n",
       "1804846    Betcha his career is not over, contrary to pop...\n",
       "1804847    I understand your attitude on this.  But I thi...\n",
       "1804848    Women's rights?  Last I checked women were jus...\n",
       "1804849    A Democrat accused of sexual harassment, go fi...\n",
       "1804850    Every time there are testimonies, like this, I...\n",
       "1804851    So you think that we should have laws protecti...\n",
       "1804852    Xi and his comrades must be smirking over Trum...\n",
       "1804853    Love your sarcasm Thick or are you just being ...\n",
       "1804854    All economists are using GDP in the world. Ind...\n",
       "1804855    It is of course normal and natural for Eugene ...\n",
       "1804856    Believing in God or not believing in God are p...\n",
       "1804857    I take your point, but I think you're shooting...\n",
       "1804858    My thought exactly.  The only people he hasn't...\n",
       "1804859    IRT Satire, fully disagree with your post.  2/...\n",
       "1804860    GBA:  I do agree that disruptive behavior shou...\n",
       "1804861    I agree, Bill G\\nThe vote-buying has begun by ...\n",
       "1804862    No, the probability of dying may be very, very...\n",
       "1804863    Nah, I am too boring to parody.  This guy Camp...\n",
       "1804864    Yes, of course that works for files on board a...\n",
       "1804865    Payette's point of view is legitimate and I th...\n",
       "1804866                 There's no whine like Alberta whine!\n",
       "1804867    If Alberta had given them nothing, you'd STILL...\n",
       "1804868    cont....GBA:  \"here's the summation of that \"l...\n",
       "1804869    Maybe the tax on \"things\" would be collected w...\n",
       "1804870    What do you call people who STILL think the di...\n",
       "1804871    thank you ,,,right or wrong,,, i am following ...\n",
       "1804872    Anyone who is quoted as having the following e...\n",
       "1804873    Students defined as EBD are legally just as di...\n",
       "Name: comment_text, Length: 1804874, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train['comment_text'].astype(str).apply(lambda x: clean_contractions(x,contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          this is so cool. it is like, 'would you want y...\n",
       "1          thank you!! this would make my life a lot less...\n",
       "2          this is such an urgent design problem; kudos t...\n",
       "3          is this something i will be able to install on...\n",
       "4                       haha you guys are a bunch of losers.\n",
       "5                                       ur a sh*tty comment.\n",
       "6                                hahahahahahahahhha suck it.\n",
       "7                                        ffffuuuuuuuuuuuuuuu\n",
       "8          the ranchers seem motivated by mostly by greed...\n",
       "9          it was a great show. not a combo i would of ex...\n",
       "10                                   wow, that sounds great.\n",
       "11         this is a great story. man. I wonder if the pe...\n",
       "12            this seems like a step in the right direction.\n",
       "13         it is ridiculous that these guys are being cal...\n",
       "14         this story gets more ridiculous by the hour! a...\n",
       "15         i agree; I do not want to grant them the legit...\n",
       "16         interesting. i will be curious to see how this...\n",
       "17                           awesome! I love civil comments!\n",
       "18         i am glad you are working on this, and I look ...\n",
       "19         angry trolls, misogynists and racists\", oh my....\n",
       "20         nice to some attempts to try to make comments ...\n",
       "21         one would hope that the purpose of introducing...\n",
       "22         comments will be randomly chosen and be review...\n",
       "23         she would be a major improvement for city coun...\n",
       "24         i agree! comments have so much potential to be...\n",
       "25         great question! it is one we are asked a lot. ...\n",
       "26         thanks, christa!  will you be adding any featu...\n",
       "27         our aim is actually the opposite: we want spir...\n",
       "28         thanks! we are really going to try — not only ...\n",
       "29         i applaud civil's efforts to create some new t...\n",
       "                                 ...                        \n",
       "1804844    one of the first things that they teach in law...\n",
       "1804845    i would ask ms. renzetti to be honest with her...\n",
       "1804846    betcha his career is not over, contrary to pop...\n",
       "1804847    i understand your attitude on this.  but I thi...\n",
       "1804848    women's rights?  last I checked women were jus...\n",
       "1804849    a democrat accused of sexual harassment, go fi...\n",
       "1804850    every time there are testimonies, like this, I...\n",
       "1804851    so you think that we should have laws protecti...\n",
       "1804852    xi and his comrades must be smirking over trum...\n",
       "1804853    love your sarcasm thick or are you just being ...\n",
       "1804854    all economists are using gdp in the world. ind...\n",
       "1804855    it is of course normal and natural for eugene ...\n",
       "1804856    believing in god or not believing in god are p...\n",
       "1804857    i take your point, but I think you are shootin...\n",
       "1804858    my thought exactly.  the only people he has no...\n",
       "1804859    irt satire, fully disagree with your post.  2/...\n",
       "1804860    gba:  I do agree that disruptive behavior shou...\n",
       "1804861    i agree, bill g\\nthe vote-buying has begun by ...\n",
       "1804862    no, the probability of dying may be very, very...\n",
       "1804863    nah, I am too boring to parody.  this guy camp...\n",
       "1804864    yes, of course that works for files on board a...\n",
       "1804865    payette's point of view is legitimate and I th...\n",
       "1804866                there is no whine like alberta whine!\n",
       "1804867    if alberta had given them nothing, you would s...\n",
       "1804868    cont....gba:  \"here's the summation of that \"l...\n",
       "1804869    maybe the tax on \"things\" would be collected w...\n",
       "1804870    what do you call people who still think the di...\n",
       "1804871    thank you ,,,right or wrong,,, I am following ...\n",
       "1804872    anyone who is quoted as having the following e...\n",
       "1804873    students defined as ebd are legally just as di...\n",
       "Name: comment_text, Length: 1804874, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=test['comment_text'].astype(str).apply(lambda x: clean_contractions(x,contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~`\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punct(text):\n",
    "    for sym in punct:\n",
    "            text=text.replace(sym,' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.astype(str).apply(lambda x: clean_punct(x))\n",
    "x_test=x_test.astype(str).apply(lambda x: clean_punct(x))\n",
    "y_train=np.where(train['target']>=0.5,1,0)\n",
    "# y_train=train['target'].values\n",
    "y_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack',\n",
    "                     'insult', 'threat']]\n",
    "num_aux_targets=len(y_aux_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_aux_train=y_aux_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=text.Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(list(x_train)+list(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=tokenizer.texts_to_sequences(x_train)\n",
    "x_test=tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test=sequence.pad_sequences(x_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1804874"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawl_matrix, crawl_unk_words=build_matrix(tokenizer.word_index,crawl_emb_path)\n",
    "glove_matrix, glove_unk_words=build_matrix(tokenizer.word_index,glove_emb_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix=np.concatenate([crawl_matrix,glove_matrix],axis=-1)\n",
    "del crawl_matrix\n",
    "del glove_matrix\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327573"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_features=len(tokenizer.word_index)+1\n",
    "max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(327573, 600)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del contraction_mapping\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_torch = torch.tensor(x_train, dtype=torch.long).cuda()\n",
    "# x_test_torch = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "# y_train_torch = torch.tensor(np.hstack([y_train[:, np.newaxis], y_aux_train]), dtype=torch.float32).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split=0.97\n",
    "if (mode=='run'):\n",
    "    len_train=len(x_train)\n",
    "if (mode=='test'):\n",
    "    len_train=int(len(x_train)*0.2)\n",
    "indices=np.random.permutation(len_train)    \n",
    "if (valid & (not k_fold)):    \n",
    "    x_train1=x_train[indices][:int(len_train*val_split)]\n",
    "    y_train1=y_train[indices][:int(len_train*val_split)]\n",
    "    y_aux_train1=y_aux_train[indices][:int(len_train*val_split)]\n",
    "    x_val=x_train[indices][int(len_train*val_split):]\n",
    "    y_val=y_train[indices][int(len_train*val_split):]\n",
    "    y_aux_val=y_aux_train[indices][int(len_train*val_split):]\n",
    "else:\n",
    "    if (not valid):\n",
    "        x_train1=x_train[indices]\n",
    "        y_train1=y_train[indices] \n",
    "        y_aux_train1=y_aux_train[indices]\n",
    "    if (valid & k_fold):\n",
    "        x_val=x_train[indices][int(len_train*0.2*(n_fold-1)): int(len_train*0.2*(n_fold))]\n",
    "        y_val=y_train[indices][int(len_train*0.2*(n_fold-1)): int(len_train*0.2*(n_fold))]\n",
    "        y_aux_val=y_aux_train[indices][int(len_train*0.2*(n_fold-1)):int(len_train*0.2*(n_fold))]\n",
    "        x_train1=np.concatenate((x_train[indices][:int(len_train*0.2*(n_fold-1))],\n",
    "                                x_train[indices][int(len_train*0.2*(n_fold)):]),axis=0)\n",
    "        y_train1=np.concatenate((y_train[indices][:int(len_train*0.2*(n_fold-1))],\n",
    "                                y_train[indices][int(len_train*0.2*(n_fold)):]),axis=0) \n",
    "        y_aux_train1=np.concatenate((y_aux_train[indices][:int(len_train*0.2*(n_fold-1))],\n",
    "                                 y_aux_train[indices][int(len_train*0.2*(n_fold)):]),axis=0) \n",
    "\n",
    "if (mode=='run'):\n",
    "    del x_train,y_train,y_aux_train\n",
    "    gc.collect()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data1=train_df.iloc[indices]\n",
    "# val_data=train_data1[int(m*0.2):int(m*0.4)]\n",
    "# train_data=train_data[:int(m*0.8)]\n",
    "# train_data=pd.concat([train_data1[:int(m*0.2)],train_data1[int(m*0.4):]],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_tensor=torch.tensor(x_train1, dtype=torch.long).cuda()\n",
    "del x_train1\n",
    "y_train_tensor=torch.tensor(np.hstack([np.expand_dims(y_train1,1),y_aux_train1]), \n",
    "                            dtype=torch.long).cuda()\n",
    "del y_train1\n",
    "if (valid):\n",
    "    x_val_tensor=torch.tensor(x_val, dtype=torch.long).cuda()\n",
    "    del x_val\n",
    "    y_val_tensor=torch.tensor(np.hstack([np.expand_dims(y_val,1),y_aux_val]), \n",
    "                                dtype=torch.long).cuda()\n",
    "    del y_val\n",
    "x_test_tensor=torch.tensor(x_test,dtype=torch.long).cuda()\n",
    "del x_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=data.TensorDataset(x_train_tensor,y_train_tensor)\n",
    "test_dataset=data.TensorDataset(x_test_tensor)\n",
    "if (valid):\n",
    "    val_dataset=data.TensorDataset(x_val_tensor,y_val_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader=data.DataLoader(test_dataset,batch_size=512,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialDropout(nn.Dropout2d):\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)    # (N, T, 1, K)\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n",
    "        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n",
    "        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n",
    "        x = x.squeeze(2)  # (N, T, K)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,embedding_matrix,num_aux_targets):\n",
    "        super().__init__()\n",
    "        embed_size=embedding_matrix.shape[1]\n",
    "        self.embedding=nn.Embedding(max_features,embed_size)\n",
    "        self.embedding.weight=nn.Parameter(torch.tensor(embedding_matrix,\n",
    "                                                        dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad=False\n",
    "        self.embedding_dropout = SpatialDropout(0.5)\n",
    "        self.lstm1=nn.LSTM(embed_size,lstm_units,batch_first=True,bidirectional=True)\n",
    "        self.lstm2=nn.LSTM(lstm_units*2,lstm_units,batch_first=True,bidirectional=True)\n",
    "        self.linear1 = nn.Linear(dense_hidden_units, dense_hidden_units)\n",
    "        self.linear2 = nn.Linear(dense_hidden_units, dense_hidden_units)\n",
    "        self.output=nn.Linear(dense_hidden_units,1)\n",
    "        self.aux_output=nn.Linear(dense_hidden_units,num_aux_targets)\n",
    "    def forward(self,x):\n",
    "        embedding=self.embedding(x)\n",
    "        embedding = self.embedding_dropout(embedding)\n",
    "        lstm1,_=self.lstm1(embedding)\n",
    "        lstm2,_=self.lstm2(lstm1)\n",
    "        avg_pool=torch.mean(lstm2,1)\n",
    "        max_pool,_=torch.max(lstm2,1)\n",
    "        conc=torch.cat((max_pool,avg_pool),1)\n",
    "        h_conc_linear1  = F.relu(self.linear1(conc))\n",
    "        h_conc_linear2  = F.relu(self.linear2(conc))\n",
    "        \n",
    "        hidden = conc + h_conc_linear1 + h_conc_linear2\n",
    "        \n",
    "        result = self.output(hidden)\n",
    "        aux_result = self.aux_output(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "      \n",
    " #         result=self.output(conc)\n",
    "#         aux_result=self.aux_output(conc)\n",
    "#         out=torch.cat([result,aux_result],1)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,train,val, loss_fn,optimizer,scheduler,num_epochs=5):\n",
    "    best_val_loss=1e+10\n",
    "    train_dataloader=data.DataLoader(train,batch_size=512,shuffle=True)\n",
    "    if (valid==True):\n",
    "        val_dataloader=data.DataLoader(val,batch_size=512,shuffle=True)\n",
    "        dataloaders={'train':train_dataloader,'val':val_dataloader}\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "#         print('Epoch {} '.format(epoch))\n",
    "        if (valid):\n",
    "            for phase in ['train', 'val']:\n",
    "                if (phase=='train'):\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                avg_loss=0\n",
    "\n",
    "                if (phase=='train'):\n",
    "                    scheduler.step()\n",
    "                for data1 in tqdm(dataloaders[phase], disable=False):\n",
    "                    x_train = data1[:-1]\n",
    "                    y_train = data1[-1]\n",
    "                    y_hat_train=model(*x_train).float()\n",
    "                    y_train=y_train.float()\n",
    "\n",
    "                    loss=loss_fn(y_hat_train, y_train)\n",
    "\n",
    "                    if (phase=='train'):\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    avg_loss += loss.item() / len(dataloaders[phase])\n",
    "\n",
    "                if ((phase=='val') and (avg_loss<best_val_loss)):\n",
    "                    best_model_wts=copy.deepcopy(model.state_dict())\n",
    "                    best_val_loss=avg_loss\n",
    "#                 if (phase=='train'):\n",
    "                print('Epoch {} {} loss: {:.4f}'.format(epoch,phase,avg_loss))\n",
    "#                 else:\n",
    "#                     print('Val loss: {:.4f} \\n'.format(avg_loss))\n",
    "        elif (not valid):\n",
    "            model.train()\n",
    "            scheduler.step()\n",
    "            avg_loss=0\n",
    "            for data1 in tqdm(train_dataloader, disable=False):\n",
    "                x_train = data1[:-1]\n",
    "                y_train = data1[-1]\n",
    "                y_hat_train=model(*x_train).float()\n",
    "                y_train=y_train.float()\n",
    "\n",
    "                loss=loss_fn(y_hat_train, y_train)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                avg_loss += loss.item() / len(train_dataloader)\n",
    "\n",
    "            if (avg_loss<best_val_loss):\n",
    "                best_model_wts=copy.deepcopy(model.state_dict())\n",
    "                best_val_loss=avg_loss\n",
    "\n",
    "            print('Epoch {} {} loss: {:.4f}'.format(epoch,'train',avg_loss))\n",
    "                    \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('Time: {:.2f} \\n'.format(elapsed_time))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(model,test_loader,output_dim):\n",
    "    preds=[]\n",
    "    test_preds = np.zeros((len(test), output_dim))\n",
    "    model.eval()\n",
    "    batch_size=test_loader.batch_size\n",
    "    for i, x_batch in enumerate(test_loader):\n",
    "            \n",
    "            y_pred = sigmoid(model(*x_batch).detach().cpu().numpy())\n",
    "\n",
    "            test_preds[i * batch_size:(i+1) * batch_size, :] = y_pred\n",
    "    return test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model  0\n",
      "Epoch 0 train loss: 0.0253\n",
      "Epoch 0 val loss: 0.0215\n",
      "Time: 517.70 \n",
      "\n",
      "Epoch 1 train loss: 0.0211\n",
      "Epoch 1 val loss: 0.0209\n",
      "Time: 517.71 \n",
      "\n",
      "Epoch 2 train loss: 0.0203\n",
      "Epoch 2 val loss: 0.0206\n",
      "Time: 520.09 \n",
      "\n",
      "Epoch 3 train loss: 0.0199\n",
      "Epoch 3 val loss: 0.0202\n",
      "Time: 517.34 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model=NeuralNet(embedding_matrix,y_aux_train1.shape[-1])\n",
    "model=model.cuda()\n",
    "loss=nn.BCEWithLogitsLoss(reduction='mean')\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "scheduler=torch.optim.lr_scheduler.LambdaLR(optimizer,lambda epoch: 0.6**epoch)\n",
    "val_input=None\n",
    "if (valid):\n",
    "    val_input=val_dataset\n",
    "all_test_preds=[]    \n",
    "for model_idx in range(num_models):\n",
    "    print('Model ', model_idx)\n",
    "    seed_everything(1234 + model_idx-1)    \n",
    "    model=train_model(model,train_dataset,val_input,loss,optimizer,scheduler,num_epochs=4)\n",
    "    test_preds = model_predict(model,test_dataloader,output_dim= y_train_tensor.shape[-1])\n",
    "    all_test_preds.append(test_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test['id'],\n",
    "    'prediction': np.mean(all_test_preds, axis=0)[:, 0]\n",
    "})\n",
    "\n",
    "submission.to_csv('submission'+str(n_fold)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
